<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example: Binary Neural Network · Bender.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://rasmuskh.github.io/Bender.jl/notebooks/Binaryexample/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Bender.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../functionindex/">functionindex</a></li><li><a class="tocitem" href="../FAexample/">Example: Feedback Alignment</a></li><li class="is-active"><a class="tocitem" href>Example: Binary Neural Network</a><ul class="internal"><li><a class="tocitem" href="#Handling-dependencies"><span>Handling dependencies</span></a></li><li><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li><li><a class="tocitem" href="#Defining-the-training-loop"><span>Defining the training loop</span></a></li><li class="toplevel"><a class="tocitem" href="#Training-the-model"><span>Training the model</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Example: Binary Neural Network</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example: Binary Neural Network</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Rasmuskh/Bender.jl/blob/master/docs/src/notebooks/Binaryexample.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Binary-Neural-Network"><a class="docs-heading-anchor" href="#Binary-Neural-Network">Binary Neural Network</a><a id="Binary-Neural-Network-1"></a><a class="docs-heading-anchor-permalink" href="#Binary-Neural-Network" title="Permalink"></a></h1><p>This page contains a short tutorial on how to train a binary neural network. It is loosely adapted from the Flux model zoo&#39;s MLP example.</p><p>We will train neural network with binary {-1,+1} weights and hidden layer activations. The output layer will have real-valued neurons. binary networks require less storage and memory space and when deployed on edge computing devices they can be more computationally effecient than real-valued networks (not so much on traditional CPUs and GPUs which have much better optimized BLAS and CUDA kernels for Float32 operations). </p><p>To create a network with binary weights we need to specify a forward mapping which applies a binary activation function to the weights, when we create the layers. We will also specify a binary activation function to ba applied to the layers output. The tricky part about training binary neural networks is that a binary function such as <code>sign(x)</code> has derivative zero everywhere except at the oprigin where it is undefined. A typical workaround is to only use the sign function during the forward pass and replace it with a differentiable surrogate such as <code>tanh</code> or <code>identity</code> during the backwards pass. This <em>hack</em> actually works quite well and a lot of research on binary neural networks explores variations of this idea. We will use the <em>straight-through estimator</em>, which is explained by Courbariaux et al in the <a href="https://arxiv.org/abs/1511.00363">BinaryConnect</a> paper. During a forward pass it acts as the sign function, but during the backwards pass (i.e. during backprop) it is replaced by the identity function.</p><p>In <em>Bender.jl</em> there is a stocastic and a deterministic variant of the STE, <code>stoc_sign_STE</code>  and <code>sign_STE</code>. We will make use of the deterministic version here. In a traditional fully connected layer the output is computed by first computing some preactivation <span>$a=Wx+b$</span> and then applying an elementwise activation function <span>$z=f\circ a$</span>. You can specify a forward mapping for the computation of <span>$a$</span> when initializing a GenDense layer, which is how we can make the layer use the straight through estimator. We will use the following mapping (which is also exported by Bender.jl)</p><pre><code class="language-julia hljs">function linear_binary_weights(a, x)
    W, b, = a.weight, a.bias
    return sign_STE.(W)*x .+ b
end</code></pre><p>Using this forward mapping we define a 3 layer fully connected network.</p><pre><code class="language-julia hljs">function build_model()
	m = Chain(  GenDense( 	784=&gt;300, sign_STE, forward=linear_binary_weights),
                GenDense( 	300=&gt;100, sign_STE; forward=linear_binary_weights), 
                GenDense(  	100=&gt;10; forward=linear_binary_weights))
    return m
end</code></pre><p>The following sections contain some boilerplate code. You can expand them to show the details.</p><h2 id="Handling-dependencies"><a class="docs-heading-anchor" href="#Handling-dependencies">Handling dependencies</a><a id="Handling-dependencies-1"></a><a class="docs-heading-anchor-permalink" href="#Handling-dependencies" title="Permalink"></a></h2><details><summary>show details</summary><pre><code class="language-julia hljs">using Pkg; Pkg.add(url=&quot;https://github.com/Rasmuskh/Bender.jl.git&quot;)</code></pre><pre><code class="language-julia hljs">begin
	using Bender, Flux, MLDatasets
	using Flux: onehotbatch, onecold, logitcrossentropy, throttle
	using Flux.Data: DataLoader
	using Parameters: @with_kw
	using DataFrames
end</code></pre></details><h2 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h2><details><summary>show details</summary><pre><code class="language-julia hljs">@with_kw mutable struct Args
    η::Float64 = 0.0003     # learning rate
    batchsize::Int = 64     # batch size
    epochs::Int = 10        # number of epochs
    device::Function = gpu  # set as gpu, if gpu available
end</code></pre><pre><code class="language-julia hljs">function getdata(args)
    ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = &quot;true&quot;

    # Loading Dataset	
    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)
    xtest, ytest = MLDatasets.MNIST.testdata(Float32)

    # Reshape Data in order to flatten each image into a vector
    xtrain = Flux.flatten(xtrain)
    xtest = Flux.flatten(xtest)

    # One-hot-encode the labels
    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)

    # Batching
    train_data = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=true, partial=false)
    test_data = DataLoader((xtest, ytest), batchsize=args.batchsize, partial=false)

    return train_data, test_data
end</code></pre><pre><code class="language-julia hljs">function evaluate(data_loader, model)
    acc = 0
	l = 0
	numbatches = length(data_loader)
    for (x,y) in data_loader
        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))*1 / size(x,2)
		l += logitcrossentropy(model(x), y)
    end
    return acc/numbatches, l/numbatches
end</code></pre></details><h2 id="Defining-the-training-loop"><a class="docs-heading-anchor" href="#Defining-the-training-loop">Defining the training loop</a><a id="Defining-the-training-loop-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-training-loop" title="Permalink"></a></h2><details><summary>show details</summary><pre><code class="language-julia hljs">function train(; kws...)
    # Initializing Model parameters 
    args = Args(; kws...)

	# Create arrays for recording training metrics
	acc_train = zeros(Float32, args.epochs)
	acc_test = zeros(Float32, args.epochs)
	loss_train = zeros(Float32, args.epochs)
	loss_test = zeros(Float32, args.epochs)
	
    # Load Data
    train_data,test_data = getdata(args)

    # Construct model
    m = build_model()
	
    train_data = args.device.(train_data)
    test_data = args.device.(test_data)
    m = args.device(m)
    loss(x,y) = logitcrossentropy(m(x), y)
    
    # Training
    opt = ADAM(args.η)
	for epoch=1:args.epochs	
        Flux.train!(loss, params(m), train_data, opt)
		acc_train[epoch], loss_train[epoch] = evaluate(train_data, m)
		acc_test[epoch], loss_test[epoch] = evaluate(test_data, m)
		# clamp weights to lie in the range [-1,+1]
		for layer in m
			layer.weight .= clamp.(layer.weight, -1, 1)
		end
    end

	# Return trianing metrics as a DataFrame
	df = DataFrame([loss_train, loss_test, acc_train, acc_test], 
				   [:loss_train, :loss_test, :acc_train, :acc_test])
	return df, cpu(m)
end</code></pre></details><h1 id="Training-the-model"><a class="docs-heading-anchor" href="#Training-the-model">Training the model</a><a id="Training-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-model" title="Permalink"></a></h1><p>Although we constrained the model to use {-1,+1} weights during inference it still manages to solve the problem fairly well. The train function stores the model&#39;s loss and accuracy on the train and test set in a DataFrame.</p><pre><code class="nohighlight hljs">df, m = train(epochs=10); round.(df, digits=3)</code></pre><table><tr><th style="text-align: center"></th><th style="text-align: center">loss_train</th><th style="text-align: center">loss_test</th><th style="text-align: center">acc_train</th><th style="text-align: center">acc_test</th></tr><tr><td style="text-align: center"></td><td style="text-align: center">Float32</td><td style="text-align: center">Float32</td><td style="text-align: center">Float32</td><td style="text-align: center">Float32</td></tr><tr><td style="text-align: center">1</td><td style="text-align: center">1.9</td><td style="text-align: center">1.862</td><td style="text-align: center">0.789</td><td style="text-align: center">0.792</td></tr><tr><td style="text-align: center">2</td><td style="text-align: center">1.56</td><td style="text-align: center">1.542</td><td style="text-align: center">0.824</td><td style="text-align: center">0.83</td></tr><tr><td style="text-align: center">3</td><td style="text-align: center">1.365</td><td style="text-align: center">1.361</td><td style="text-align: center">0.85</td><td style="text-align: center">0.849</td></tr><tr><td style="text-align: center">4</td><td style="text-align: center">1.362</td><td style="text-align: center">1.382</td><td style="text-align: center">0.846</td><td style="text-align: center">0.852</td></tr><tr><td style="text-align: center">5</td><td style="text-align: center">1.294</td><td style="text-align: center">1.34</td><td style="text-align: center">0.859</td><td style="text-align: center">0.859</td></tr><tr><td style="text-align: center">6</td><td style="text-align: center">1.234</td><td style="text-align: center">1.224</td><td style="text-align: center">0.866</td><td style="text-align: center">0.87</td></tr><tr><td style="text-align: center">7</td><td style="text-align: center">1.268</td><td style="text-align: center">1.269</td><td style="text-align: center">0.865</td><td style="text-align: center">0.869</td></tr><tr><td style="text-align: center">8</td><td style="text-align: center">1.306</td><td style="text-align: center">1.395</td><td style="text-align: center">0.857</td><td style="text-align: center">0.855</td></tr><tr><td style="text-align: center">9</td><td style="text-align: center">1.222</td><td style="text-align: center">1.26</td><td style="text-align: center">0.873</td><td style="text-align: center">0.873</td></tr><tr><td style="text-align: center">10</td><td style="text-align: center">1.22</td><td style="text-align: center">1.263</td><td style="text-align: center">0.867</td><td style="text-align: center">0.867</td></tr></table></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../FAexample/">« Example: Feedback Alignment</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.15 on <span class="colophon-date" title="Tuesday 3 May 2022 13:05">Tuesday 3 May 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
