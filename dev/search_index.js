var documenterSearchIndex = {"docs":
[{"location":"functionindex/#Index","page":"functionindex","title":"Index","text":"","category":"section"},{"location":"functionindex/","page":"functionindex","title":"functionindex","text":"Modules = [Bender]","category":"page"},{"location":"notebooks/FAexample/#Feedback-Alignment","page":"Example: Feedback Alignment","title":"Feedback Alignment","text":"","category":"section"},{"location":"notebooks/FAexample/#Handling-dependencies","page":"Example: Feedback Alignment","title":"Handling dependencies","text":"","category":"section"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"using Pkg; Pkg.add(url=\"https://github.com/Rasmuskh/Bender.jl.git\")","category":"page"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"begin\n\tusing Bender, Flux, MLDatasets\n\tusing Flux: onehotbatch, onecold, logitcrossentropy, throttle\n\tusing Flux.Data: DataLoader\n\tusing Parameters: @with_kw\n\tusing DataFrames\nend","category":"page"},{"location":"notebooks/FAexample/#Utilities","page":"Example: Feedback Alignment","title":"Utilities","text":"","category":"section"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"@with_kw mutable struct Args\n    η::Float64 = 0.0003     # learning rate\n    batchsize::Int = 64     # batch size\n    epochs::Int = 10        # number of epochs\n    device::Function = gpu  # set as gpu, if gpu available\nend","category":"page"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"function getdata(args)\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n    # Loading Dataset\t\n    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)\n    xtest, ytest = MLDatasets.MNIST.testdata(Float32)\n\n    # Reshape Data in order to flatten each image into a vector\n    xtrain = Flux.flatten(xtrain)\n    xtest = Flux.flatten(xtest)\n\n    # One-hot-encode the labels\n    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n\n    # Batching\n    train_data = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=true, partial=false)\n    test_data = DataLoader((xtest, ytest), batchsize=args.batchsize, partial=false)\n\n    return train_data, test_data\nend","category":"page"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"function evaluate(data_loader, model)\n    acc = 0\n\tl = 0\n\tnumbatches = length(data_loader)\n    for (x,y) in data_loader\n        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))*1 / size(x,2)\n\t\tl += logitcrossentropy(model(x), y)\n    end\n    return acc/numbatches, l/numbatches\nend","category":"page"},{"location":"notebooks/FAexample/#Defining-the-training-loop","page":"Example: Feedback Alignment","title":"Defining the training loop","text":"","category":"section"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"function train(; kws...)\n    # Initializing Model parameters \n    args = Args(; kws...)\n\n\t# Create arrays for recording training metrics\n\tacc_train = zeros(Float32, args.epochs)\n\tacc_test = zeros(Float32, args.epochs)\n\tloss_train = zeros(Float32, args.epochs)\n\tloss_test = zeros(Float32, args.epochs)\n\t\n    # Load Data\n    train_data,test_data = getdata(args)\n\n    # Construct model\n    m = build_model()\n    train_data = args.device.(train_data)\n    test_data = args.device.(test_data)\n    m = args.device(m)\n    loss(x,y) = logitcrossentropy(m(x), y)\n    \n    # Training\n    opt = ADAM(args.η)\n\tfor epoch=1:args.epochs\t\n        Flux.train!(loss, params(m), train_data, opt)\n\t\tacc_train[epoch], loss_train[epoch] = evaluate(train_data, m)\n\t\tacc_test[epoch], loss_test[epoch] = evaluate(test_data, m)\n    end\n\n\t# Return trianing metrics as a DataFrame\n\tdf = DataFrame([loss_train, loss_test, acc_train, acc_test], \n\t\t\t\t   [:loss_train, :loss_test, :acc_train, :acc_test])\n\treturn df\nend","category":"page"},{"location":"notebooks/FAexample/#Defining-the-model","page":"Example: Feedback Alignment","title":"Defining the model","text":"","category":"section"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"function build_model()\n    m = Chain(GenDense(784=>128, 128=>784, relu; forward=linear_asym_∂x),\n              GenDense(128=>64, 64=>128, relu; forward=linear_asym_∂x),\n              GenDense(64=>10, 10=>64; forward=linear_asym_∂x))\n    return m\nend","category":"page"},{"location":"notebooks/FAexample/#Training-the-model","page":"Example: Feedback Alignment","title":"Training the model","text":"","category":"section"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"df = train(epochs=10); round.(df, digits=3)","category":"page"},{"location":"notebooks/FAexample/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":" loss_train loss_test acc_train acc_test\n1 0.418 0.413 0.881 0.884\n2 0.317 0.318 0.909 0.91\n3 0.269 0.274 0.923 0.922\n4 0.232 0.238 0.934 0.932\n5 0.201 0.208 0.943 0.941\n6 0.176 0.187 0.951 0.946\n7 0.155 0.169 0.956 0.951\n8 0.138 0.156 0.96 0.955\n9 0.125 0.145 0.964 0.956\n10 0.113 0.136 0.967 0.959","category":"page"},{"location":"#Bender.jl","page":"Home","title":"Bender.jl","text":"","category":"section"},{"location":"#Layers","page":"Home","title":"Layers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GenDense","category":"page"},{"location":"#Bender.GenDense","page":"Home","title":"Bender.GenDense","text":"Generalized version of Flux's Dense layer. The forward keyword allows you to choose the form of the forward mapping.\n\nGenDense(in=>out, σ=identity; \n         init = glorot_uniform, \n         bias=true, γ=Flux.Zeros(), forward=linear)\n\nCan also be initialized with an additional set of trainable weights \n\nGenDense(in=>out, in_asym=>out_asym, σ = identity; \n         init = glorot_uniform, \n         bias=true, bias_asym=true, γ=Flux.Zeros(), forward=linear)\n\nTODO: add examples.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"GenConv","category":"page"},{"location":"#Bender.GenConv","page":"Home","title":"Bender.GenConv","text":"Generalized version of Flux's conv layer\n\n\n\n\n\n","category":"type"},{"location":"#Similarity/correlation-functions","page":"Home","title":"Similarity/correlation functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"matmul","category":"page"},{"location":"#Bender.matmul","page":"Home","title":"Bender.matmul","text":"Regular matrix multiplication.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"matmul_asym_∂x","category":"page"},{"location":"#Bender.matmul_asym_∂x","page":"Home","title":"Bender.matmul_asym_∂x","text":"Compute matrix multiplication, but takes an additional matrix B as input.  B has same dims as Wᵀ, and is used in the backwards pass.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"matmul_blocked_∂x","category":"page"},{"location":"#Bender.matmul_blocked_∂x","page":"Home","title":"Bender.matmul_blocked_∂x","text":"Matrix multiplication with custom rrule\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"radialSim","category":"page"},{"location":"#Bender.radialSim","page":"Home","title":"Bender.radialSim","text":"Compute negative squared euclidean distance D between the rows of matrix W and the columns of matrix X. Denoting the rows of W by index i and the columns of X by index j the elements of the output matrix is given by: Dᵢⱼ = -||Wᵢ﹕ - X﹕ⱼ||² = 2Wᵢ﹕X﹕,j - ||Wᵢ﹕||^2 - ||X﹕ⱼ||².\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"radialSim_asym","category":"page"},{"location":"#Bender.radialSim_asym","page":"Home","title":"Bender.radialSim_asym","text":"In the forward pass this function behaves just like radialSim, but in the backwards pass weight symmetry is broken by using matrix B rather than Wᵀ. See docstring for radialSim for more details.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"conv_asym_∂x","category":"page"},{"location":"#Bender.conv_asym_∂x","page":"Home","title":"Bender.conv_asym_∂x","text":"computes the convolution of image x with kernel w when called, but uses a different set of weights w_asym to compute the pullback wrt x. This is typically uses in feedback alignment experiments.\n\n\n\n\n\n","category":"function"},{"location":"#Loss-functions","page":"Home","title":"Loss functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"direct_feedback_loss","category":"page"},{"location":"#Bender.direct_feedback_loss","page":"Home","title":"Bender.direct_feedback_loss","text":"Error function which takes a vector of the hidden and output neurons states as well as a vector of feedback matrices as arguments\n\n\n\n\n\n","category":"function"},{"location":"#Activation-functions","page":"Home","title":"Activation functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"sign_STE","category":"page"},{"location":"#Bender.sign_STE","page":"Home","title":"Bender.sign_STE","text":"Deterministic straight-through estimator for the sign function. References: https://arxiv.org/abs/1308.3432, https://arxiv.org/abs/1511.00363\n\n\n\n\n\n","category":"function"},{"location":"notebooks/Binaryexample/#Binary-Neural-Network","page":"Example: Binary Neural Network","title":"Binary Neural Network","text":"","category":"section"},{"location":"notebooks/Binaryexample/#Handling-dependencies","page":"Example: Binary Neural Network","title":"Handling dependencies","text":"","category":"section"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"using Pkg; Pkg.add(url=\"https://github.com/Rasmuskh/Bender.jl.git\")","category":"page"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"begin\n\tusing Bender, Flux, MLDatasets\n\tusing Flux: onehotbatch, onecold, logitcrossentropy, throttle\n\tusing Flux.Data: DataLoader\n\tusing Parameters: @with_kw\n\tusing DataFrames\nend","category":"page"},{"location":"notebooks/Binaryexample/#Utilities","page":"Example: Binary Neural Network","title":"Utilities","text":"","category":"section"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"@with_kw mutable struct Args\n    η::Float64 = 0.0003     # learning rate\n    batchsize::Int = 64     # batch size\n    epochs::Int = 10        # number of epochs\n    device::Function = gpu  # set as gpu, if gpu available\nend","category":"page"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"function getdata(args)\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n    # Loading Dataset\t\n    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)\n    xtest, ytest = MLDatasets.MNIST.testdata(Float32)\n\n    # Reshape Data in order to flatten each image into a vector\n    xtrain = Flux.flatten(xtrain)\n    xtest = Flux.flatten(xtest)\n\n    # One-hot-encode the labels\n    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n\n    # Batching\n    train_data = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=true, partial=false)\n    test_data = DataLoader((xtest, ytest), batchsize=args.batchsize, partial=false)\n\n    return train_data, test_data\nend","category":"page"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"function evaluate(data_loader, model)\n    acc = 0\n\tl = 0\n\tnumbatches = length(data_loader)\n    for (x,y) in data_loader\n        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))*1 / size(x,2)\n\t\tl += logitcrossentropy(model(x), y)\n    end\n    return acc/numbatches, l/numbatches\nend","category":"page"},{"location":"notebooks/Binaryexample/#Defining-the-training-loop","page":"Example: Binary Neural Network","title":"Defining the training loop","text":"","category":"section"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"function train(; kws...)\n    # Initializing Model parameters \n    args = Args(; kws...)\n\n\t# Create arrays for recording training metrics\n\tacc_train = zeros(Float32, args.epochs)\n\tacc_test = zeros(Float32, args.epochs)\n\tloss_train = zeros(Float32, args.epochs)\n\tloss_test = zeros(Float32, args.epochs)\n\t\n    # Load Data\n    train_data,test_data = getdata(args)\n\n    # Construct model\n    m = build_model()\n    train_data = args.device.(train_data)\n    test_data = args.device.(test_data)\n    m = args.device(m)\n    loss(x,y) = logitcrossentropy(m(x), y)\n    \n    # Training\n    opt = ADAM(args.η)\n\tfor epoch=1:args.epochs\t\n        Flux.train!(loss, params(m), train_data, opt)\n\t\tacc_train[epoch], loss_train[epoch] = evaluate(train_data, m)\n\t\tacc_test[epoch], loss_test[epoch] = evaluate(test_data, m)\n    end\n\n\t# Return trianing metrics as a DataFrame\n\tdf = DataFrame([loss_train, loss_test, acc_train, acc_test], \n\t\t\t\t   [:loss_train, :loss_test, :acc_train, :acc_test])\n\treturn df\nend","category":"page"},{"location":"notebooks/Binaryexample/#Defining-the-model","page":"Example: Binary Neural Network","title":"Defining the model","text":"","category":"section"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"We now define a model which uses binary weights and binary activation functions (except for the output layer which we allow to have real valued activations).","category":"page"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"function build_model()\n\tm = Chain(  GenDense(784=>128, sign_STE; forward=linear_binary_weights),\n                GenDense(128=>64, sign_STE; forward=linear_binary_weights),\n                GenDense(64=>10; forward=linear_binary_weights))\n    return m\nend","category":"page"},{"location":"notebooks/Binaryexample/#Training-the-model","page":"Example: Binary Neural Network","title":"Training the model","text":"","category":"section"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":"df = train(epochs=10); round.(df, digits=3)","category":"page"},{"location":"notebooks/Binaryexample/","page":"Example: Binary Neural Network","title":"Example: Binary Neural Network","text":" loss_train loss_test acc_train acc_test\n1 1.866 1.796 0.73 0.742\n2 1.502 1.487 0.792 0.799\n3 1.34 1.369 0.816 0.815\n4 1.26 1.233 0.824 0.828\n5 1.274 1.236 0.821 0.823\n6 1.168 1.14 0.84 0.845\n7 1.168 1.122 0.846 0.852\n8 1.138 1.155 0.842 0.847\n9 1.108 1.105 0.848 0.853\n10 1.039 1.053 0.864 0.869","category":"page"}]
}
