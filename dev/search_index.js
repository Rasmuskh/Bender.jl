var documenterSearchIndex = {"docs":
[{"location":"#RogueLearning.jl","page":"Home","title":"RogueLearning.jl","text":"","category":"section"},{"location":"#Function-Documentation","page":"Home","title":"Function Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modules = [RogueLearning]\nOrder   = [:function, :type]","category":"page"},{"location":"#ChainRulesCore.rrule-Tuple{typeof(RogueLearning.matmul_dfa), AbstractMatrix, AbstractMatrix}","page":"Home","title":"ChainRulesCore.rrule","text":"This rrule blocks the regular backprop pathway by returning NoTangent() instead of ∂X\n\n\n\n\n\n","category":"method"},{"location":"#ChainRulesCore.rrule-Tuple{typeof(conv), Any, Any, Any, Any}","page":"Home","title":"ChainRulesCore.rrule","text":"When conv is called with an additional weight matrix wFB, then we will use this custom rrule, which transports error backwards using wFB instead of W.\n\n\n\n\n\n","category":"method"},{"location":"#ChainRulesCore.rrule-Tuple{typeof(dfa_loss), Any, AbstractMatrix, Any, Any}","page":"Home","title":"ChainRulesCore.rrule","text":"This rrule pipes output errors directly back into the hidden layers via the feedback matrices Bᵢ. To use it use Flux.activations to get hidden layers activation states and pass the array of hidden states in as the argument x along with an array of appropriately dimensioned feedback matrices.\n\n\n\n\n\n","category":"method"},{"location":"#ChainRulesCore.rrule-Tuple{typeof(matmul), AbstractMatrix, AbstractMatrix, AbstractMatrix}","page":"Home","title":"ChainRulesCore.rrule","text":"rrule which uses feedback weights B instead of Wᵀ, which regular backpropagation would use.\n\n\n\n\n\n","category":"method"},{"location":"#NNlib.conv-Union{Tuple{N}, Tuple{T}, Tuple{Any, AbstractArray{T, N}, AbstractArray{T, N}, Any}} where {T, N}","page":"Home","title":"NNlib.conv","text":"conv called with an additional feedback weight matrix. The forward pass gives the same results, but in the backwards pass the gradient will be different thanks to multiple dispatch.\n\n\n\n\n\n","category":"method"},{"location":"#RogueLearning.dfa_loss-NTuple{4, Any}","page":"Home","title":"RogueLearning.dfa_loss","text":"Error function which takes a vector of the hidden and output neurons states as well as a vector of feedback matrices as arguments\n\n\n\n\n\n","category":"method"},{"location":"#RogueLearning.matmul-Tuple{Any, Any, Any}","page":"Home","title":"RogueLearning.matmul","text":"Compute matrix multiplication, but takes an additional matrix B as input.  B has same dims as Wᵀ, and is used in the backwards pass.\n\n\n\n\n\n","category":"method"},{"location":"#RogueLearning.matmul-Tuple{Any, Any}","page":"Home","title":"RogueLearning.matmul","text":"Regular matrix multiplication.\n\n\n\n\n\n","category":"method"},{"location":"#RogueLearning.matmul_dfa-Tuple{Any, Any}","page":"Home","title":"RogueLearning.matmul_dfa","text":"Matrix multiplication with custom rrule\n\n\n\n\n\n","category":"method"},{"location":"#RogueLearning.radialSim-Tuple{Any, Any, Any}","page":"Home","title":"RogueLearning.radialSim","text":"In the forward pass this function behaves just like radialSim, but in the backwards pass weight symmetry is broken by using matrix B rather than Wᵀ. See docstring for radialSim for more details.\n\n\n\n\n\n","category":"method"},{"location":"#RogueLearning.radialSim-Tuple{Any, Any}","page":"Home","title":"RogueLearning.radialSim","text":"Compute negative squared euclidean distance D between the rows of matrix W and the columns of matrix X. Denoting the rows of W by index i and the columns of X by index j the elements of the output matrix is given by: Dᵢⱼ = -||Wᵢ﹕ - X﹕ⱼ||² = 2Wᵢ﹕X﹕,j - ||Wᵢ﹕||^2 - ||X﹕ⱼ||².\n\n\n\n\n\n","category":"method"},{"location":"#RogueLearning.GenConv","page":"Home","title":"RogueLearning.GenConv","text":"Generalized version of Flux's conv layer\n\n\n\n\n\n","category":"type"},{"location":"#RogueLearning.GenDense","page":"Home","title":"RogueLearning.GenDense","text":"Generalized version of Flux's Dense layer.     GenDense(in=>out, σ=identity; ω = identity, ψ = *, init = glorotuniform, bias=true, γ=Flux.Zeros() Can also be initialized with an additional set of trainable weights asym     GenDense(in=>out, inasym=>outasym, σ = identity; ω = identity, ψ = *, init = glorotuniform, bias=true, bias_asym=true, γ=Flux.Zeros())\n\n\n\n\n\n","category":"type"}]
}
