var documenterSearchIndex = {"docs":
[{"location":"functionindex/#Index","page":"functionindex","title":"Index","text":"","category":"section"},{"location":"functionindex/","page":"functionindex","title":"functionindex","text":"Modules = [Bender]","category":"page"},{"location":"notebooks/FA_notebook/","page":"Example: Feedback Alignment","title":"Example: Feedback Alignment","text":"<style>\n    table {\n        display: table !important;\n        margin: 2rem auto !important;\n        border-top: 2pt solid rgba(0,0,0,0.2);\n        border-bottom: 2pt solid rgba(0,0,0,0.2);\n    }\n\n    pre, div {\n        margin-top: 1.4rem !important;\n        margin-bottom: 1.4rem !important;\n    }\n\n    .code-output {\n        padding: 0.7rem 0.5rem !important;\n    }\n</style>\n\n<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"a87eef967f608486f49f61ca99d167b432e6bbe66155d836b2d9d2bb6e5e732d\"\n    julia_version = \"1.7.2\"\n-->\n\n<div class=\"markdown\"><h1>Direct Feedback Alignment</h1>\n</div>\n\n\n<div class=\"markdown\"><h2>Handling Dependencies</h2>\n</div>\n\n<pre class='language-julia'><code class='language-julia'>using Pkg</code></pre>\n\n\n<pre class='language-julia'><code class='language-julia'>Pkg.add(url=\"https://github.com/Rasmuskh/Bender.jl.git\")</code></pre>\n\n\n<pre class='language-julia'><code class='language-julia'>begin\n    using Bender, Flux, MLDatasets, Statistics, CUDA\n    using Flux: onehotbatch, onecold, logitcrossentropy, throttle\n    using Flux.Data: DataLoader\n    using Parameters: @with_kw\n    using DataFrames\nend</code></pre>\n\n\n\n<div class=\"markdown\"><h2>Utilities</h2>\n</div>\n\n<pre class='language-julia'><code class='language-julia'>@with_kw mutable struct Args\n    η::Float64 = 0.0003#3e-4       # learning rate\n    batchsize::Int = 64   # batch size\n    epochs::Int = 10        # number of epochs\n    device::Function = gpu  # set as gpu, if gpu available\nend</code></pre>\n<pre id='var-@unpack_Args' class='code-output documenter-example-output'>Args</pre>\n\n<pre class='language-julia'><code class='language-julia'>function getdata(args)\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n    # Loading Dataset\t\n    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)\n    xtest, ytest = MLDatasets.MNIST.testdata(Float32)\n    \n    # Reshape Data in order to flatten each image into a vector\n    xtrain = Flux.flatten(xtrain)\n    xtest = Flux.flatten(xtest)\n\n    # One-hot-encode the labels\n    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n\n    # Batching\n    train_data = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=true, partial=false)\n    test_data = DataLoader((xtest, ytest), batchsize=args.batchsize, partial=false)\n\n    return train_data, test_data\nend</code></pre>\n<pre id='var-getdata' class='code-output documenter-example-output'>getdata (generic function with 1 method)</pre>\n\n<pre class='language-julia'><code class='language-julia'>function evaluate(data_loader, model)\n    acc = 0\n    l = 0\n    numbatches = length(data_loader)\n    for (x,y) in data_loader\n        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))*1 / size(x,2)\n        l += logitcrossentropy(model(x), y)\n    end\n    return acc/numbatches, l/numbatches\nend</code></pre>\n<pre id='var-evaluate' class='code-output documenter-example-output'>evaluate (generic function with 1 method)</pre>\n\n\n<div class=\"markdown\"><h2>Defining the Training Loop</h2>\n</div>\n\n<pre class='language-julia'><code class='language-julia'>function train(; kws...)\n    # Initializing Model parameters \n    args = Args(; kws...)\n\n    # Create arrays for recording training metrics\n    acc_train = zeros(Float32, args.epochs)\n    acc_test = zeros(Float32, args.epochs)\n    loss_train = zeros(Float32, args.epochs)\n    loss_test = zeros(Float32, args.epochs)\n    \n    # Load Data\n    train_data,test_data = getdata(args)\n\n    # Construct model\n    m = build_model()\n    train_data = args.device.(train_data)\n    test_data = args.device.(test_data)\n    m = args.device(m)\n    loss(x,y) = logitcrossentropy(m(x), y)\n    \n    # Training\n    opt = ADAM(args.η)\n    for epoch=1:args.epochs\t\n        Flux.train!(loss, params(m), train_data, opt)\n        acc_train[epoch], loss_train[epoch] = evaluate(train_data, m)\n        acc_test[epoch], loss_test[epoch] = evaluate(test_data, m)\n    end\n\n    # Return trianing metrics as a DataFrame\n    df = DataFrame([loss_train, loss_test, acc_train, acc_test], \n                   [:loss_train, :loss_test, :acc_train, :acc_test])\n    return df\nend</code></pre>\n<pre id='var-train' class='code-output documenter-example-output'>train (generic function with 1 method)</pre>\n\n\n<div class=\"markdown\"><h2>Defining the model</h2>\n</div>\n\n<pre class='language-julia'><code class='language-julia'>function build_model()\n    m = Chain(  GenDense(784=&gt;512, 512=&gt;784, relu; forward=linear_asym_∂x),\n                GenDense(512=&gt;256, 256=&gt;512, relu; forward=linear_asym_∂x),\n                GenDense(256=&gt;10, 10=&gt;256; forward=linear_asym_∂x)\n            )\n    return m\nend</code></pre>\n<pre id='var-build_model' class='code-output documenter-example-output'>build_model (generic function with 1 method)</pre>\n\n<pre class='language-julia'><code class='language-julia'>df = train(epochs=10)</code></pre>\n<table>\n<tr>\n<th>loss_train</th>\n<th>loss_test</th>\n<th>acc_train</th>\n<th>acc_test</th>\n</tr>\n<tr>\n<td>0.266108</td>\n<td>0.259388</td>\n<td>0.922442</td>\n<td>0.92498</td>\n</tr>\n<tr>\n<td>0.181013</td>\n<td>0.183927</td>\n<td>0.947789</td>\n<td>0.946114</td>\n</tr>\n<tr>\n<td>0.133806</td>\n<td>0.145075</td>\n<td>0.960946</td>\n<td>0.954527</td>\n</tr>\n<tr>\n<td>0.101212</td>\n<td>0.118843</td>\n<td>0.970284</td>\n<td>0.963842</td>\n</tr>\n<tr>\n<td>0.0818218</td>\n<td>0.105554</td>\n<td>0.975337</td>\n<td>0.96865</td>\n</tr>\n<tr>\n<td>0.0687234</td>\n<td>0.0982712</td>\n<td>0.978922</td>\n<td>0.971154</td>\n</tr>\n<tr>\n<td>0.0586691</td>\n<td>0.0940164</td>\n<td>0.981457</td>\n<td>0.971955</td>\n</tr>\n<tr>\n<td>0.0500062</td>\n<td>0.0915856</td>\n<td>0.984458</td>\n<td>0.973658</td>\n</tr>\n<tr>\n<td>0.0426535</td>\n<td>0.0911335</td>\n<td>0.986476</td>\n<td>0.973157</td>\n</tr>\n<tr>\n<td>0.0378737</td>\n<td>0.0935544</td>\n<td>0.987877</td>\n<td>0.973458</td>\n</tr>\n</table>\n\n\n<!-- PlutoStaticHTML.End -->","category":"page"},{"location":"#Bender.jl","page":"Home","title":"Bender.jl","text":"","category":"section"},{"location":"#Layers","page":"Home","title":"Layers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GenDense","category":"page"},{"location":"#Bender.GenDense","page":"Home","title":"Bender.GenDense","text":"Generalized version of Flux's Dense layer. The forward keyword allows you to choose the form of the forward mapping.\n\nGenDense(in=>out, σ=identity; \n         init = glorot_uniform, \n         bias=true, γ=Flux.Zeros(), forward=linear)\n\nCan also be initialized with an additional set of trainable weights \n\nGenDense(in=>out, in_asym=>out_asym, σ = identity; \n         init = glorot_uniform, \n         bias=true, bias_asym=true, γ=Flux.Zeros(), forward=linear)\n\nTODO: add examples.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"GenConv","category":"page"},{"location":"#Bender.GenConv","page":"Home","title":"Bender.GenConv","text":"Generalized version of Flux's conv layer\n\n\n\n\n\n","category":"type"},{"location":"#Similarity/correlation-functions","page":"Home","title":"Similarity/correlation functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"matmul","category":"page"},{"location":"#Bender.matmul","page":"Home","title":"Bender.matmul","text":"Regular matrix multiplication.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"matmul_asym_∂x","category":"page"},{"location":"#Bender.matmul_asym_∂x","page":"Home","title":"Bender.matmul_asym_∂x","text":"Compute matrix multiplication, but takes an additional matrix B as input.  B has same dims as Wᵀ, and is used in the backwards pass.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"matmul_blocked_∂x","category":"page"},{"location":"#Bender.matmul_blocked_∂x","page":"Home","title":"Bender.matmul_blocked_∂x","text":"Matrix multiplication with custom rrule\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"radialSim","category":"page"},{"location":"#Bender.radialSim","page":"Home","title":"Bender.radialSim","text":"Compute negative squared euclidean distance D between the rows of matrix W and the columns of matrix X. Denoting the rows of W by index i and the columns of X by index j the elements of the output matrix is given by: Dᵢⱼ = -||Wᵢ﹕ - X﹕ⱼ||² = 2Wᵢ﹕X﹕,j - ||Wᵢ﹕||^2 - ||X﹕ⱼ||².\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"radialSim_asym","category":"page"},{"location":"#Bender.radialSim_asym","page":"Home","title":"Bender.radialSim_asym","text":"In the forward pass this function behaves just like radialSim, but in the backwards pass weight symmetry is broken by using matrix B rather than Wᵀ. See docstring for radialSim for more details.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"conv_asym_∂x","category":"page"},{"location":"#Bender.conv_asym_∂x","page":"Home","title":"Bender.conv_asym_∂x","text":"computes the convolution of image x with kernel w when called, but uses a different set of weights w_asym to compute the pullback wrt x. This is typically uses in feedback alignment experiments.\n\n\n\n\n\n","category":"function"},{"location":"#Loss-functions","page":"Home","title":"Loss functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"direct_feedback_loss","category":"page"},{"location":"#Bender.direct_feedback_loss","page":"Home","title":"Bender.direct_feedback_loss","text":"Error function which takes a vector of the hidden and output neurons states as well as a vector of feedback matrices as arguments\n\n\n\n\n\n","category":"function"},{"location":"#Activation-functions","page":"Home","title":"Activation functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"sign_STE","category":"page"},{"location":"#Bender.sign_STE","page":"Home","title":"Bender.sign_STE","text":"Deterministic straight-through estimator for the sign function. References: https://arxiv.org/abs/1308.3432, https://arxiv.org/abs/1511.00363\n\n\n\n\n\n","category":"function"}]
}
